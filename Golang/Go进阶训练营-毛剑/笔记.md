- TCP/IP 书籍 《UNIX Network Programming》
- 网络命令：
- nload tcpflow（监听端口 查看 header 头部信息） ss netstat nmon top 
- 全链路工具
- About Jeager 阿里有使用
- 微服务设计：
- API Geteway 老接口 新接口的兼容 每个服务的API数据不统一
- 分散服务 -》 返回聚合场景 
- 发送时保守 接收时开放
- 1.2  底层中去做 4g wifi 电视端 类型 升级比较不合理
- 微服务搬到了 BFF层
- 底层微服务不考虑终端的场景
- 账户 积分 分久必合 合久必分 
- CQRS
- 稿件审核服务 稿件结果服务 状态的转变放在了审核稿件来做 
- 查询是否能播放 放在了稿件结果服务中
- 1.3 grpc 同步方面的知识 服务发现 
- 客户端发现  服务提供者 和 服务消费者直连
- Endpoint 端点
- Client Snapshot
- 参考 Eureka 实现自己的 AP 发现服务 
- 1.4 多租户 多集群 
- 多租户 发布测试 多个测试环境 
- 从源头传递一个标签，比方说 http head  然后挂载在 go 的 context 上下文中 
- 然后基于 RPC 的负载均衡流量来做路由 路由到你想要的节点
- 好处 只搭建一套测试环境 可以虚拟出无数套并行的测试环境
- 错误检查：
- 2.2 Error Type
- 输出时候 加上统一前缀：包名
- 程序不能依赖 error.Error 的输出 是给程序员使用的 程序不是这样用的
- 预定义的Error 无法携带更多的返回信息 
- 不依赖检查 Error.Error 做等值判断
- 2.3 github.com/pkg/errors
- 内存
- 对于变量的赋值 尽量使用 atorm.value
- 你觉得满足原子性 但是不满足可见性 关于 cpu 层面的理论
- COW copy only write 
- 锁的底层：barrier 或者 atomic CAS 指令
- MESI CPU 如何通知其他的核心去把内存给刷掉的
- 3.3  Package sync
- go build -race 
- go test -race
- sync.pool
- 打日志
- 3.4 Package context
- 使用场景之一：
- 聊天房间的消息发送 可以使用带 buffer 的 消息积攒一定的量后 再发送出去 
- 日志 刷盘的场景 
- 关闭的时候 一定保证没有写入的时候 才能关闭 不然关闭后再写入会 panic
- go context withValue 不适合大量创建 withValue 重新赋值的时候 应该拷贝一个新的
- context 不要放在结构体中 http 是一个特例 http 本身就是一个请求
- 4.2  API 设计 
- 服务而非对象，消息而非引用
- API 仓库 IDL 文件定义 即文档的方式
- 破坏语义 删除字段 方法 更新大版本
- 错误码的唯一性 是跟服务绑定的
- 4.3  配置管理
- 配置使用指针传递 可以修改配置的值 使用普通变量 不能修改配置的值 ？ 待测试
- yaml -> json -> protobuf
- 4.4 模块单元测试
- 工程化 使用docker 快速部署环境 启动测试的一些注意点
- beer-shop demo
- 5.1 隔离
- 服务隔离 false sharing 
- 演唱会中 直播摄像头电源拔掉 用户刷新页面
- 使用消息队列 kafka 和 部门 绑定 做 隔离 存储数据中 需要保存数据到三个地方 mysql redis es
- 主动监控房间人数 到一定量 比如 5万 后 广播到其他的服务 做 cache 等于本地 local 数据也同样 local
- go 中的 goroution io 都是 托管 runtime 管理着
- 视频时长 的 隔离 超大时长的视频 容易造成堵塞 其他短视频 资源占用
- 局部故障 变成 全局故障  图片压缩 缩略图 gif 比较慢 做隔离
- 数据库 group 比较慢 做隔离 根据sql语句做隔离
- 发布版本后 日志 info 记录的多 导致队列积压 日志 全部都要看 错误 info 不能只看 error级别的错误
- 5.2 超时控制
- 进入机房 添加设备 手握了一下网线 内网网络问题
- 内网使用山寨的 没有使用原装的 导致网络问题
- 整个服务响应更慢
- 恶性循环：内存涨的很快的时候 gc 也会介入 gc 介入后 cpu 也会高
- Quota 超时：层级传递，跨进程跨线程传递下去  进程内进程外全局继承 取 Quota 和 设置的超时 min 值
- 配置nginx 超时 数据库连接池超时 连接超时 读超时 写超时
- 5.3 过载保护 bbr 限流
- 令牌桶和漏斗算法 设置的峰值不好掌控，根据多个节点，接口的版本更新 响应时间会变化 
- 利特尔法则 
- 限流加超时加过载保护 这几个机制 结合 空出时间 给人工介入
- 针对不同的租户设置限制
- 一个请求 这个请求本身就存在问题，参数10000 导致运行时间增加 或者程序bug 程序死循环
- 分布式限流：
- 常规做法是 使用redis inc 加1
- 使用每次去拿100 总的QPS 是 1000 然后拿到后 在节点本地 使用令牌桶算法 做到限流
- 每次拿多少 比较难配置 过去的监控qps 去拿多少 刚开始设置一个初始值
- 最大最小公平分享 Max-Min Fairness 优先满足最小最少分配
- 按照接口去配置限流 粒度太细 按照服务配置限流 粒度太粗 
- 接口分优先级等级
- 熔断是对下游保护 场景：下游出现了大量的错误时候 自适应限流是对自己保护
- 技能冷却中
- 如果一个客户疯狂的请求对外的接口 要做客户的限流 值为0 不让请求
- 5.4 降级
- 是容量预估不准确，导致的，不能经常的降级。
- 手动降级：比如：bibli 评论不能用，评论置灰
- AI 推荐不可用，可用推荐系统热门的数据给客户展示
- 用户进入评论，目的是观看评论 可以使用用户的头像和名称使用系统默认展示
- 定期数据缓存 返回用户上次的数据
- local cache  多份保存 redis 场景：当刚上线 需要返回local cache local cache 没有数据 展示用户白屏。
- 重试 级联风暴 多层级的重试传递
- 写操作 不建议重试  读操作可以重试 
- 5.5 负载均衡 p2c 算法
- 局部负载 是正确的，考虑到全局负载 就存在偏差的问题
- 解决方案 p2c 随机选取两个节点来打分
- 站在后端角度就是要对方的负载
- 站在前端角度 clent 角度是最真正体现RPC质量的，包括健康度 包括QPS 包括latency 真正体现服务质量
- 如果存在一个每次比分都差的节点 那么这个节点就不会有流量进入，解决方案是 比较长时间适当的放一些流量。查看cpu 
- 6.1 评论系统 架构设计 == 数据流转的设计
- 设计：缓存设计  消息队列  存储设计
- 避免出现环
- 避免出现双向请求
- 重上到下
- 读：
- 读多写少 
- 分层
- common
- common-server 数据的评论处理 楼层怎么构建 楼中楼怎么设计  缓存回填 最早做在本层 回填数据增多 会导致 oom 解决方法：消息发送给 kfaka common-job 处理 如- 果有需要回填 redis 则 common-job 处理 （接收一个很少的消息 然后处理redis）
- kfaka partition 多个 如果热点数据 导致 某一个 partition 成为热点 该怎么处理？
-  策略多变 等级 先发后审 先审后发 排序 评论 删除 某些用户 可以发 不可以发
- 数据的读写 是稳定的
- common-job
- 读写分开 不同的服务 处理不同
- 处理 审核 删除 不直接操作数据库 而是先从 es 获取重要信息 再去操作数据库
- 6.2 存储和可用性设计
- 评论关联表 和 内容表的id 保持一致
- 先插入内容表，然后再 事务更新 评论主表和关联表 如果事务失败，也没有关系。就是多一条数据
- 评论索引表中  root 根评论id，不为0是回复评论id  parent 父评论id，为0是root评论
- 
-  设计的巧妙 
- 归并回源：一个干活 全部共享结果 对应的包 singleflight 解决缓存穿透非常重要的一个点
- 缓存构建的过程需要1,2秒 在这时间内还有相同主题的请求打过来
- kafka 存入 common-job 丢弃 
- 在一个进程内用一个 short-lived的 flag 就是进程内的一个LRU 进程内很短的缓存。
- 控制kfaka 回源命令的数量 节点数量 就是 查询数量 数据库压力减少
- 如果某一个主题很热门 程序中订阅请求量 主题 做一个滑动窗口 5秒内 有哪些key 被高频的访问
- 设计一个环形数组存 5秒内的数据 ，每5秒 数据生成一 小根堆 查询第k位的热点key 然后 缓存到 local cache 注意 local cache 也控制量 内存也是有限的
- 7.1 历史记录价格设计 功能模块与架构设计
- write-back 写频率非常高的情况下，缓存一层，有一个队列消费缓存 缓存可以做重排，持久层有一定的速率写入
- 写入Kafka 时候 聚合处理请求 打包和pipline的流式处理 
- 7.2 存储设计与实现
- 节点上线或者下线会影响之前的key 导致多个版本存在
- 导致多个版本的原因，节点下线后，key重新hash 存到之前的节点，当下线的节点重新上线后，之前的key 还是会存在，又可能存在多个版本，数据不一致
- 历史架构：
- BFF层做请求参数打包，common-job 做解压和聚合优化
- redis 用户历史记录 -》用户历史信息如果redis没有，则不去查询 Hbase，舍弃一些不重要的
- 用户是否登录，优化在App端 head 中 传入当前的日期，服务端对比日期，如果不等，则写入到用户 二进制数据类型中
- 8.1 分布式缓存  CRDT 解决缓存一致性问题
- 微服务中绑定redis集群，
- redis中加一层代理 管理redis 类似于 redis-cliter 使用的是 twernproxy 自己研发的 overload
- 一致性hash 如果新增节点对于之前的节点分布数据不均匀
- 对于相邻节点有优势，优化后，对于节点新增虚拟节点，做节点和虚拟节点的映射关系 A（A#1 A#2 A#3）可以优化成，多虚拟节点的使用 key 分布均匀
- memcache 优势
- 整个 value 比较大 存储大文本 多线程，吞吐比较不错，取大文本的时候，比较有优势。
- 红包设计
- 一般使用悲观锁，使用一致性hash 一次节点计算好，一次更新数据库（聚合计算后的）
- 合并红包，一次更新更多的数据行数，
- 有界一致性hash 算法
- 节点过载情况下，飘过到下一个节点
- redis 分 16384个槽位
- 优点：迁移数据的情况下，当槽位分布节点 （0~1024 节点1）的情况下槽位占用范围，key 迁移会按照槽位来迁移。我想应该比较容易管理，槽位的大小是固定的
- 数据一致性 重试次数 同步DB 同步Cache job消息重试更新补偿
- job读操作回填，写入和回填新增优先级，如果回填 add 有值情况 setnx 有值则跳过 没有则写入
- 二级缓存与数据一致性：
- 注意：多级缓存 扇出 一次性优先清理 下游，再清理 acount_server 层
- 下游缓存的expire 要大于上游，里面穿透回源 情况
- 热点缓存
- 很高频的读 都读多写少 例如：banner
- 小表广播：各个进程每一个微服务的进程自己利用 in-process的cache 提升为 local cache
- 主动发现热点，自动提升为 local cache
- 当多服务副本的情况 更新热点key 先删除 再写入，
- 当热点key 需要更新的时候 会透传到mysql 导致 mysql压力增大 解决方案 在删除key的是 不是直接删除而是挪到其他的一个地方 保存10秒 返回中标识已过期的key 前端- 判断是否使用过期的key
-  用归并回源放 cache miss 的消息，同时用消息队列消费的时候要做一个与判定，key是否存在，不需要重新构建，对于穿透缓存有优势
- 租约：lease 
- 不要让大量的包发送出去 打包的大小要根据 上一个请求 延迟有多少，自动把pipeline的数据增加或减少
- 如果latency 非常好 pipeline 很高
- 为什么产生大量的大包发送过来？
- 网络产生抖动 内网产生延迟  很可能产生延迟 大量的节点都会 cache miss
- 
- 
- 8.2 分布式事务 Sage 玩法 同步事务 
- 先付钱后取货 付钱和取货 分开
- Transactional outbox 事务性的收件箱 发起后 本次存入 message表 
- 账本表 message_apply 表 记录 另一个服务 接受到的消息的状态 保证幂等性
- 
- 最大交付 努力送达 如果消费者失败 则需要人工介入了
- try 新增预扣款的操作 tcc
- 防空回滚 防悬挂 try 和 confirm 执行的顺序一定要保证顺序 不能 try 失败后 confirm执行，try 再成功
- 9.2 Go语言实战-网络编程 Goim 长链接网关
- 负载均衡 地区-》根据负载的监控情况 返回 接口的 ip 列表 发送探测包 检测 ttl 时间 优先排序
- 客户端会优先使用上一次链接的 ip 如果第一个地址连接失败 则请求第二个
- 客户端会判断 是否更换网络 切换 wifi 等 方式 主动触发 请求ip列表地址
- 日志&指标&链路追踪 日志 解决的是明细的查询
- 日志在 dosomething 中 err.warp 包装后 往上层抛， 在middleware 拦截器里面捕获 请求的 middleware 中
- 日志不能强依赖第三方库 应该做一层映射 解耦 无论底层怎么切换 我和依赖的代码一行都不用该
- 日志使用 ELK 使用 otel 规范
- 10.2 链路跟踪 解决性能问题 以及故障点 热点 利用 Trae Id 和 Span id 跟日志系统进行结合
- 应对积极采样  高QPS 采样低 低QPS 采样高
- 固定采样 做成滑动窗口式的采样
- 二级采样：服务节点多了，采样样本多 丢弃的策略不能丢弃同一个 traceid 的 span
- 使用hash 算法 把 traceid 转成一个标量Z 这里 0<=Z<=1 采用 一个固定的 大于0.2 则保存 小于则丢弃
- 下游采样
- 通过 traceid spanid 和 parentid 还原整个概览日志 
- 关键用户的日志 全采样 
- 容量预估 上游扩大1倍 下游进行对应的扩容 
- 10.3 指标 是汇总 是看趋势 
- 延迟 流量 错误 饱和度
- 11.1 DNS & CDN & 多活架构 
- CDN&DNS 就是为了流量路由调度服务的
- 多活 核心就是 sharding key 按什么业务去划分
- 小运营商 节省资源 转发到了其他运营商 做了代理
- LocalDNS 和你 出口 NET的运营商 不是一个地区 解析结果不是最佳的
- 路由 一个IP 对应的是 一组服务器 稳定的 Anycast 实现 离最近的服务 
- CDN 选最近的节点  先到边缘节点 再到核心机房 缓存静态资源 缓存静态文件 nginx 负载均衡
- COSS 环形存储 存储文件 
- 多活系统
- 业务分级
- 数据分级
- 数据同步
- 异常处理
- global 视频资源 首页 视频详情 用户登录鉴权
- 分片 弹幕 非核心业务
- 蚂蚁金服架构
- 单活架构：具体的用户运行在虚拟的 单节点
- 双活架构：同城或者临城的区域 实现双活 主备架构
- 冷备架构：不同区域的 数据异步同步。会有延迟。数据会存在弱一致
- 饿了么架构
- 根据区域 DNS & CND 路由 到对应的区域
- 根据 用户 商家 骑手 区域 去 分片到具体的 区域节点
- 每个节点包含所有的 服务 餐饮店 骑手 用户收单地址 不需要跨机房 跨服务调用
- 阿里
- Gzone RZone CZone GZone 其实是 CZone的 主 RZone 是 单元内的节点 某一个数据的分片 
- 中心机房：做数据的复制转发的 复制链路不会想对等网络这么复杂
- 数据库 五副本 必须写成功 三副本 才返回成功
- 苏宁多活
- 其中一点：竞争proxy 例如：库存数量 每次拿 50 ~ 100 的数量 消费完后再去请求 这样的思路 减少请求次数
- Facebook 多活 还有一种机制 TAO
- 
- 微信朋友圈异地多活
- 因果一致性算法
- 全局消息id + 合并后 再广播到其他节点  全量冗余
- 账户多活 
- 账户注册是不能多活的 会存在宕机情况下 数据不一致性 只能做成 全局公用服务
- 登录和鉴权的场景 可以做多活 二次读取
### 12 消息队列 Kfaka Topic&partition
#### 使用场景：
- 数据分析，访问量，pb pv, 实时计算，
- 推荐阅读 DDIA 中文名 数据密集型应用系统设计
- 时间 O(1) 的访问，快速扩容。自动扩缩容。
- 用户登录后 需要积分记录 用户和积分不是强关联的 解耦后 登录后发消息给 积分系统 积分系统监听服务 消费消息即可
#### Topic 主题
- Broker kfaka 服务节点
- partition 分区
- 实验条件 3个 Broker 1个 Topic 无Replication 异步模式 3个 Producer 消息 Payload 为 100字节
- - 为什么会快，存储与文件系统之上。顺序读写。缓存利用内存。利用linux page cache 的优化。
- 文件格式按照 Topic 和 Partition 数量组合分目录的。后缀 .index 是索引文件，.log 是数据文件。查找，文件二分查找找到文件，然后，通过索引（稀疏索引），offset 定位偏移量，找数据。
- 当 Partition 数量小于 Broker 个数时，Partition 数量越大，吞吐率越高，且呈线性提升
- Producer 发送 & Consumer 消费 
- Kfaka 超大集群 会存在性能消减的情况 主要的是顺序IO 和 pack cache 
- 多个 consumer group 慢的 和 快的 会产生争用，读取老的 pack cache 和 新的 pack cache 会影响性能
- 下游消费很慢 导致kfaka 负载partition 切换 解决方案是 绑定 partition 
- 消费端 拉去 服务端的 offset 
- 消费端 消费有两种机制
- 至多一次消费：读完消息 先 commit 再处理消息 
- 读完消息，先处理消息 再commit  业务需要处理幂等
- Flik 结合 Kfaka 二阶段提交 先写临时目录 再提交 结合 offset id 拉去没有 commit 落盘的数据
- Leade & Follower
- Raft 多副本
- 
- Consumer只能读取到 High watermark 因为 Follower 还没有完全落盘到 Log end offset
- Follower 和 Leade 会存在数据不一致的情况，解决方法是：加入了 <epoch, offset> 一起定义数据是否一致 还是会存在数据丢失的情况。
- 数据不一致的原因，leader 和 Follower 的服务不可用，主从切换。数据HW 后的 丢弃的原因 

13 Runtime
- GMP
- 让一个 goroutine 绑在一个 M上执行
- 在M1 预热了，然后又跑到了M2 执行，G 调到同一个 M 的概率不高
- 自旋：
- 保留一个自旋的M 保证新的G很快被运行
- 唤醒或者创建一个M 如果M从空闲变成活跃，意味着肯能一个处于自旋状态的M进入工作状态了，这时要检查并确保还有一个自旋M存在，以防还有G或者还有P空着的。
- 内存
- 栈分配很廉价 堆分配很昂贵 堆的gc 需要看是否有引用 引用是否有存活。然后再gc掉 腾出空间
- 堆上的内存不能返回的，返回会报错， 
- 交叉引用，如果一个变量在该函数的堆栈帧，其他地方还有引用，则放在堆上，堆上的变量，则不容易释放掉，等系统的gc 才会清理。
- go build -gcflags '-m' 告诉你哪些变量会逃逸的，分配到栈上的
- runtime.morestack





